{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c55d152a-e5e9-44cb-a042-6527ac177d15",
     "showTitle": true,
     "title": "Azure Data Lake storage Access"
    }
   },
   "source": [
    " \n",
    "- Each storage container comes with a set of Access keys that one can use to read and write data which are called as Storage Access Keys.\n",
    "- They also come with a Shared Access Signature token or SAS Token that can be used to maintain the the access to the user at a more granular level than the Storage Access Keys.\n",
    "- Service Principal can be assigned with particular permission to access the datalake and then credentials of this service princiapl can be used to access the datalake from databricks\n",
    "\n",
    "The above mentioned methods to access the data lake container can be achecived from databricks using \n",
    "  - Session Scoped Authentication \n",
    "    - Usage of the above credentials is within the notebook.\n",
    "    - This access would tied to that session i.e., until the notebook is detached from the cluster \n",
    "  \n",
    "  - Cluster Scoped Authentication\n",
    "    - Usage of the credentials at the cluster level and would be valid until the cluster is terminated\n",
    "\n",
    "- Through Azure Active Directory (AAD)\n",
    "  - AAD Passthrough Authentication: where the databricks cluster looks at the credentials of user and based on his roles assigned through the IAM( Identity Access Management) will be able to parse through the data the user has access to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce41bc0e-5ca9-4f85-9bdd-d82b7555beb5",
     "showTitle": true,
     "title": "Steps involved in creating a Azure Data lake Storage "
    }
   },
   "source": [
    "1) Click on the hamburger icon on the top left corner of the console. </br>\n",
    "2) Create a Resource and search for storage account. </br>\n",
    "3) Select the subscription from teh dropdown for billing purposes. </br>\n",
    "4) Select the resource group under which the storage account should be created for. </br>\n",
    "5) Enter the nema of the storage container nad the region closest to you and also the redundacy method for maintaining a HA storage account.</br>\n",
    "    - there are four types of redundancies to choose from:\n",
    "      - LRS (Local Redundant Storage)\n",
    "      - ZRS (Zone Redundant Storage)\n",
    "      - GRS (Geo Redundant Storage)\n",
    "      - GZRS (Geo Zone Redundant Storage)\n",
    "6) Click next on to Advanced settings and to enable the Datalake storage (ADLS v2) capability of the storage container select the tick mark the option to enable Hierarchical Namespace.\n",
    "7) Leaving everything as it is click on review and create the storage account "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebc734f8-ec72-41f0-88ce-290a3efe8b72",
     "showTitle": true,
     "title": "Information about a storage account "
    }
   },
   "source": [
    "- Each storage account comes two Access Keys\n",
    "  - Has full access to the storage account (Super User).\n",
    "  - Recommended secure using Azure Key Vault.\n",
    "  - If Key is compromissed/lost they can rotated or regenerated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef68a89c-a90b-4d55-99a8-1cacc17b4240",
     "showTitle": true,
     "title": "Access Keys "
    }
   },
   "source": [
    " setting up a spark configuration on Azure Databricks using the below command would help databeicks access the data stored in the data container\n",
    " \n",
    " spark.conf.set(\"fs.azure.account.key.\\<storage-account\\>.dfs.core.windows.net\",\"\\<access-key\\>\") </br>\n",
    " spark.conf.set(config.endpoint,access_key)\n",
    "\n",
    " - Azure Recommends to use **Azure Blob File System (ABFS)** driver to access data in the containers rather than the https protocol \n",
    "\n",
    " URI Syntax \n",
    " - abfss://container@storage_account_name.dfs.core.windows.net/folder_path/file_name \n",
    "  - Ex. abfss://demo@f1azda.dfs.core.windows.net/circuits.csv\n",
    "  - Ex. abfss://demo@f1azda.dfs.core.windows.net/test/circuits.csv\n",
    "\n",
    "Once we have the URI we can use the dbutils.fs.ls command along with the URI to access the data within the storage container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b6e392-e13e-4216-beca-bf39e249ee2c",
     "showTitle": true,
     "title": "Shared Access Signature Tokens"
    }
   },
   "outputs": [],
   "source": [
    "Provides more fine grained access to the data in the storage accounts by:\n",
    "-   Restricting access to only specific type of files or only access specific containers in the storage account.\n",
    "-   Allowing specific permissions to the user like the user is only allowed to read data within the files but not update/write.\n",
    "-   Restricting the users to access the storage accounts only at a given time period of the day.\n",
    "-   Limiting the access to the data to a range or specific IP addresses.\n",
    "-   Best Practice to share SAS tokes with External clients who dont have direct access to the Environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0138fb30-195d-4cd9-bfe3-8e2a4f460aad",
     "showTitle": true,
     "title": "Service Principle"
    }
   },
   "source": [
    "Service Priniciples are similar to users which can be defined in the Azure Active Directory and then be assigned RBAC roles to assign permissions/ level of access to various Azure Services \n",
    "- Recommended approach to give access to Services during automation processes such as in CI/CD pipelines or automated job runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a52550d6-1306-4e12-87ea-5a24f33d2084",
     "showTitle": true,
     "title": "Cluster Based Access"
    }
   },
   "source": [
    "Above Discussed access methods are valid only for a given session that is valid only till the notebook is detached from from teh cluster or until the cluster automatically terminates.\n",
    "So we can assign variables during cluster startup so that the authentication is done during cluster initialization and all the notebooks attahced to the cluster or users accessing the cluster can access the resources (for ex. directly access the data in the data lake) without any setup in the notebook.\n",
    "\n",
    "But this approach is not a good approach in the case of a large organisation as individual team should have thier cluster launched as the application for each team will be different and this architecture would difficult to manage as the team/ organisation grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "632852b3-9141-4c89-bd88-205ea0667662",
     "showTitle": true,
     "title": "Securing keys using Azure and Databricks "
    }
   },
   "source": [
    "Databricks --> Secret Scopes and in Azure --> Azure Key Vault\n",
    "\n",
    "Secret Scopes are a way through which credentials can be stored in databricks securily by referencing them in clusters, notebooks and jobs\n",
    "- Two types of Scopes \n",
    "    - Databricks backed Secret Scope --> This is a solution which is backed and owned by Databricks and can be accessed by Databricks CLI \n",
    "    - Azure Key Vault backed Secret Scope (Recommended approach while using Databricks on Azure)\n",
    "\n",
    "## Linking Azure Key Vault to Databricks \n",
    "\n",
    "- Create a Azure Key vault Resource --> Link it to the Databricks backed Scope --> reference the secrets using the databricks secrets utility **dbutils.secrets.get**\n",
    "\n",
    "## Process \n",
    "\n",
    "- Step 1: Copy the token values from the azure container (Access key token, SAS Token)\n",
    "- Step 2: Go to Azure key Vault and create a new Vault\n",
    "- Step 3: Within the newly created Vault --> go to Secrets option in the side menu bar --> import/generate a new secret (top menu bar) --> Give the secret a name and secret value (Like a Key Value pair).\n",
    "- Step 4: In the key Vault --> Go to Properties under settings in the side menu --> make a note of the **Vault URI** and **Resource ID**.\n",
    "- Step 5: In the databricks homepage modify the URL to include **#secrets/createScope** at the end and also make sure that you are not there on onboarding page i.e., in the URI after **azuredatabtricks.net/onboarding?o=** --> **incorrect** and it should be **azuredatabtricks.net/?o=**.\n",
    "- Step 6: In the create secret Scope menu give a name to the secret scope and then also provide the details such as **Azure Vault URI** and **Resource ID** to link the Azure Key Vault to the Databricks Secret Scope feature.\n",
    "- Step 7a: To get the list of Scopes available in the Secret Scope database --> dbutils.secrets.listScopes()\n",
    "- Step 7b: To get the list of Secrets within the scope --> dbutils.secrets.list(scope = \"\\<Scope Name>\")\n",
    "- Step 7c: To get the information from a particular scope and an element in the list --> dbutils.secrets.get(scope = \"\\<Scope Name>\", key = \"\\<Key Value>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfbd25d-108c-4caf-b1e7-0958ee52ec22",
     "showTitle": true,
     "title": "Spark Architecture "
    }
   },
   "source": [
    "- Comprised of **driver and Worker nodes** and a **cluster manager node**\n",
    "  -  The Driver is repsonsible for setting up the sparkContext which acts as an Entry point into the spark cluster \n",
    "  -  Once the program is submitted the logical plan for execution is created at the driver node.\n",
    "  -  the logical plan is made until by the **DAG Scheduler** until an action keyword is called and based on the differnet transformations till then the plan is then split into stages which consist of different tasks.\n",
    "  -  Based on the dataset it is operating the driver node does negotiates with the cluster manager on allocation of resources(worker nodes) to complete the tasks.\n",
    "  -  The Cluster manager spins up the worker nodes with the help of the **Task Scheduler** and communicates the job to be performed by them.\n",
    "  -  The status of the jobs are reported periodically to the cluster manager and incase of the a worker node failure the cluster manager will spin up a new cluster and reassign the task the lost node as assigned.\n",
    "\n",
    "References:\n",
    "- https://www.interviewbit.com/blog/apache-spark-architecture/\n",
    "- https://www.databricks.com/glossary/catalyst-optimizer\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/understand-the-internal-working-of-apache-spark/\n",
    "- https://stackoverflow.com/questions/24909958/spark-on-yarn-concept-understanding/38598830#38598830\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ffd961c-fa2b-4599-9649-1bdd7abb95e2",
     "showTitle": true,
     "title": "Running another notebook from within a notebook "
    }
   },
   "source": [
    "- Can use the %run command to run another notebook from within another one and use the variables or functions from the \n",
    "- For Example Syntax can be like: </br>\n",
    "  **%run ../path/to/file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a004b1bb-871e-4687-86a4-c24ce97cae72",
     "showTitle": true,
     "title": "Passing parameters using widgets option of dbutils"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.<option>\n",
    "\n",
    "dbutils.widgets.help --> gets us the manual on how to use the various other methods which can be used to configure widgets. \n",
    "dbutlis.widgets.text(\"param_name\",\"default_val\") --> creates a text box on top of the page on what parameter value can be passed through.\n",
    "dbutils.widgets.get(\"param_name\") --> get the value entered in the parameter above created to bring it into the context of the workbook.\n",
    "\n",
    "and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62eae0af-6e96-486b-bd1a-e6e44d11f9fb",
     "showTitle": true,
     "title": ""
    }
   },
   "source": [
    "\n",
    "- Hive Metastore is a registry which stores information about the metadata of the files in a tablular manner which includes details such as location of the file, file extension etc.\n",
    "\n",
    "- Apache Hive provides a metastore for Spark to use while query.\n",
    "\n",
    "#### Spark Implementation\n",
    "\n",
    "- When SQL query is querying a table from a spark context it first checks for the metadata in the hive catalog and if it is present the data \n",
    "\n",
    "- Types of tables that can be created using the spark sql are Managed and external where\n",
    "  - Managed table is a kind of table where the metadata and table data is handled by Spark SQL itself \n",
    "  - External table is a kind of table where the only the metadata is handled by spark and data is handled by the user\n",
    "\n",
    "- **NOTE: When a managed table is deleted by spark the metadata and the data is deleted and when an external table is deleted, metadata only gets deleted where as the data still remains as is**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb6a19b2-5f3f-459d-9b4c-1a87eca128ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Delta Lake\n",
    "  - This is a datalake with ACID transaction controls which enables delta table to ingest streaming data along with the batch load processing.\n",
    "\n",
    "  - There are three stages of transformation which are:\n",
    "    - raw data storage (Bronze Zone)\n",
    "    - Pre processed data storage (Silver Zone)\n",
    "    - Aggregated data storage for BI reporting (Gold Zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def19648-b782-4256-82a1-23db5b4f7da7",
     "showTitle": true,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8fdc4e4-a34d-4cd5-9b08-1ad4bf5de9d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### UPDATE\n",
    "\n",
    "1) How to update an existing delta table using SQL \n",
    "    - pretty straight forward:\n",
    "    - UPDATE TBL_NM SET COL_NAME = OPERATION WHERE CONDITION \n",
    "    - Eg:- UPDATE RESULTS_MANAGED SET POINTS = 11 - POSITION WHERE POSITION < 11\n",
    "2) How to update an existing delta table using Python \n",
    "    - import statement --> from delta.tables import DeltaTable\n",
    "    - delta_table = DeltaTable.forPath(\"/mnt/f1azda/demo/results_managed\")\n",
    "    - delta_table.update(\"position<11\",{\"points\":\"21-position\"})\n",
    "\n",
    "#### DELETE \n",
    "\n",
    "4) Delete data from delta table using SQL\n",
    "    - DELETE FROM tbl_nm where condition\n",
    "\n",
    "5) Delete data from delta table using python \\\n",
    "    from delta.tables import DeltaTable \\\n",
    "    deltatable = DeltaTable.forPath(spark,\"/mnt/f1azda/demo/result_managed\") \\\n",
    "    deltatable.delete(\"position > 10\")\n",
    "\n",
    "#### MERGE/ UPSERT(Update or Insert)\n",
    "\n",
    "6) SQL Syntax \\\n",
    "    -  MERGE INTO TGT_TBL \\\n",
    "       USING SRC_TBL \\\n",
    "       ON JOIN CONDITION \\\n",
    "       WHEN MATCHED THEN \\\n",
    "       UPDATE SET TGT_TBL.COL = SRC_TBL.COL \\\n",
    "       WHEN NOT MATCHED THEN \\\n",
    "       INSERT (TGT_TBL COLS) VALUES (SRC_TBL COLS) \\\n",
    "7) PySpark Syntax: \\\n",
    "    - from delta.tables import DeltaTable\\\n",
    "      from pyspark.sql.functions import * as f \\\n",
    "      _loading the target table_ \\\n",
    "      delta_table = DeltaTable.forPath(spark,\"path_to_storage folder\") \\\n",
    "      _defining the merge function_ \\\n",
    "      delta_table.alias(\"TGT\").merge(source_tbl.alias(\"SRC\"),\"TGT.col_name = SRC.col_name\")\\\n",
    "      .whenMatchedUpdate(set = {\"TGT.col2\" : \"SRC.col2\",\\\n",
    "                                \"TGT.col3\" : \"SRC.col3\"})\\\n",
    "      .whenNotMatchedInsert(values = {\"TGT.col1\" : \"SRC.col1\", \\\n",
    "                                      \"TGT.col1\" : \"SRC.col1\"}) \\\n",
    "      .execute()\n",
    "    \n",
    "#### TIME TRAVEL\n",
    "\n",
    "3) Time travel\n",
    "- SQL Syntax\n",
    "    - RESTORE TABLE tbl_nm TO TIMESTAMP AS OF of \"yyyy-mm-dd hh:mm:ss.ms\"\n",
    "    - To know the history of changes made to a delta table we can use:\n",
    "    -  DESCRIBE HISTORY tbl_name\n",
    "- To get the table data from a particular point in time or a specific version.\n",
    "\n",
    "    - SELECT * FROM tbl_nm VERSION AS OF 1;\n",
    "    - SELECT * FROM tbl_nm TIMESTAMP AS OF \"2024-03-20T21:49:09.000+00:00\"\n",
    "\n",
    "- Python Syntax\n",
    "    - Format to get the older version of the dataframe from the file using the version.\n",
    "    df_old = spark.read.format(\"delta\").option(\"versionAsOf\",1).load(\"/mnt/f1azda/demo/drivers_day_merge\")\n",
    "    - Format to get the older version of the dataframe from the file using the timestamp.\n",
    "    df_old = spark.read.format(\"delta\").option(\"timeStampAsOf\",1).load(\"/mnt/f1azda/demo/drivers_day_merge\")\n",
    "\n",
    "#### VACUUM METHOD TO SATISY GDPR CONSTRAINTS \n",
    "    - Vacuum is the method to delete the history of all the data and the latest data in the delta lake\n",
    "    - SQL Syntax\n",
    "        - VACUUM tbl_nm RETAIN n HOURS --> default is 168 HOURS that is 7 days\n",
    "    - To change the default to lesser than 7 days we have to set the spark parameter\n",
    "        - SET spark.databricks.delta.rententionDurationCheck.enabled = false\n",
    "\n",
    "#### Sccenario --> Data has been corrupted and it needs to be brought back to its original state\n",
    "- SQL syntax (using MERGE INTO )\n",
    "    - MERGE INTO TGT_TBL \\\n",
    "      USING SRC \\\n",
    "      ON TGT.COL = SRC.COL\\\n",
    "      WHEN NOT MATCHED \\\n",
    "      INSERT * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba4c9184-97e6-4ed1-ba7c-c4af5c350556",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Transaction log for a delta table \n",
    "1) For each operation on the table (Insert, Update, Delete) a new parquet file is created in the storage location and a corresponding \n",
    "   log transactionfile in the form of a json file.\n",
    "2) a. when a table is created it first creates a parquet file in teh given location of the database. \\\n",
    "   b. Also a file in the transaction log is created with vital information against the processs. \\\n",
    "   c. When more and more data is being generated there will be a transaction checkpoint to which the process can refer to ignoring \n",
    "      all the individaul files that represent sinlgle transactions s that happened on the table before the checkpoint. \\s\n",
    "\n",
    "3) Transaction table content when a table is created is pretty straight forward it contains the following:\n",
    "  - Transaction commit information:\n",
    "    - timestamp: when the create table statment was commited.\n",
    "    - Username: Email of the account responsible for the action.\n",
    "    - Operation: Type of CRUD operation performed (WRITE). \n",
    "    - a few other details such as notebook information, cluster information, transaction id\n",
    "  - etadata\n",
    "    - createdTime: Created time stamp\n",
    "    - paritionColumns (list): if the table is to be partitoned by any columnm it would be mentioned here \n",
    "    - schemaString: type of the table which is \\\n",
    "        - data type of table :\"struct\"\n",
    "        - field (list of dictionaries):contains information about the fields in the table such as name, type, nullable, metadata of a given column in the table \n",
    "\n",
    "4) Transaction table content when a record in a table is added :\n",
    "   -Transaction commit info\n",
    "    - timestamp: when the create table statment was commited.\n",
    "    - Username: Email of the account responsible for the action.\n",
    "    - Operation: Type of CRUD operation performed (CREATE TABLE). \n",
    "    - a few other details such as notebook information, cluster information, transaction id\n",
    "  - add (as a new parquet file is created when ever an operation is done on a delta table)\n",
    "    - path: \"name of the file being referred to\"\n",
    "    - dataChange: Boolean object stating if there is any change in the underlying data of the file.\n",
    "    - numstats (numerical stats): suggesting min and max values in each columns, numOfRecords, and also number of null values in each column is mentioned.\n",
    "\n",
    "5) Transaction table content when a record in a table is deleted :\n",
    "   -Transaction commit info\n",
    "    - timestamp: when the create table statment was commited.\n",
    "    - Username: Email of the account responsible for the action.\n",
    "    - Operation: Type of CRUD operation performed (DELETE). \n",
    "    - a few other details such as notebook information, cluster information, transaction id\n",
    "    - OperationMetrics are mentioned such as number of removed files, number of removed records, time taken to scan the table for finding the records and also time taken to rewrite the information.  \n",
    "  - remove \n",
    "    - path: \"name of the file being referred to\"\n",
    "    - dataChange: Boolean object stating if there is any change in the underlying data of the file.\n",
    "    - Deletion_timestamp --> epoch tome of when a file is deleted.  \n",
    "\n",
    "6) There is also a .crc --> Checksum Redundancy Check file which can be used to check if the data is a match. \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4f84122e-0a90-4e7a-9450-cd9b7b7d1a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Doe\n<md5 _hashlib.HASH object @ 0x7f32d42d6b50>\n4c2a904bafba06591225113ad17b5cec\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def mask_sensitive_data(input_data, field_to_mask):\n",
    "    \"\"\"\n",
    "    Mask the specified field in the input data using a hash function.\n",
    "\n",
    "    Parameters:\n",
    "    - input_data: Dictionary containing sensitive information.\n",
    "    - field_to_mask: Name of the field to be masked.\n",
    "\n",
    "    Returns:\n",
    "    - Masked data.\n",
    "    \"\"\"\n",
    "    masked_data = input_data.copy()  # Create a copy to avoid modifying the original data\n",
    "\n",
    "    if field_to_mask in masked_data:\n",
    "        original_value = masked_data[field_to_mask]\n",
    "\n",
    "        # Using SHA-256 hash for masking (you can choose a different method based on your requirements)\n",
    "        print(original_value)\n",
    "        hash_object = hashlib.md5(original_value.encode())\n",
    "        print(hash_object)\n",
    "        masked_value = hash_object.hexdigest()\n",
    "        print(masked_value)\n",
    "\n",
    "        masked_data[field_to_mask] = masked_value\n",
    "\n",
    "    return masked_data\n",
    "\n",
    "# Example Usage:\n",
    "sensitive_data = {\n",
    "    \"patient_id\": \"12345\",\n",
    "    \"patient_name\": \"John Doe\",\n",
    "    \"ssn\": \"123-45-6789\",\n",
    "    \"medical_history\": \"Sensitive information here\",\n",
    "}\n",
    "\n",
    "field_to_mask = \"patient_name\"\n",
    "\n",
    "masked_data = mask_sensitive_data(sensitive_data, field_to_mask)\n",
    "# print(\"Original Data:\", sensitive_data)\n",
    "# print(\"Masked Data:\", masked_data)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
